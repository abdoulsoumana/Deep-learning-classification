# -*- coding: utf-8 -*-
"""deep_soumana_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vC5n3mBW9zQib03WiUCv_DZYffSG_6sM
"""

from google.colab import drive
drive.mount('/content/gdrive')

import tensorflow as tf
print(tf.__version__)
print(dir(tf.feature_column))

from keras.applications.inception_v3 import InceptionV3,preprocess_input,decode_predictions
from keras.preprocessing import image
import numpy as np
from keras.layers import Dense, GlobalAveragePooling2D,Dropout,Input
# from keras.layers.advanced_activations import LeakyReLU, ELU
from keras.models import Sequential,Model
from keras import backend as K
from IPython.display import display

base_model  = InceptionV3(weights = 'imagenet', include_top=False)
print('loaded model')

#Définir le dictionnaire pour le générateur de données d'image
data_gen_args = dict(preprocessing_function=preprocess_input, 
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    vertical_flip = True)

train_datagen = image.ImageDataGenerator(**data_gen_args)
test_datagen = image.ImageDataGenerator(**data_gen_args)

train_generator = train_datagen.flow_from_directory("gdrive/My Drive//DEEP/J D/chest_xrays/train",
                                                    target_size=(28,28),batch_size=32)  

valid_generator = test_datagen.flow_from_directory("gdrive/My Drive/DEEP/J D/chest_xrays/test",
                                                    target_size=(28,28),batch_size=32)

# Install the PyDrive wrapper & import libraries.
# This only needs to be done once in a notebook.
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client.
# This only needs to be done once in a notebook.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Create & upload a text file.
uploaded = drive.CreateFile({'title': 'Sample file.txt'})
uploaded.SetContentString('Sample upload file content')
uploaded.Upload()
print('Uploaded file with ID {}'.format(uploaded.get('id')))

from google.colab import auth
auth.authenticate_user()

!pip install h5py pyyaml
!pip install tf_nightly

!pip install torch
import torch

!pip install torchvision
import torchvision

from tensorflow.keras.preprocessing.image import ImageDataGenerator

from keras.layers.convolutional import Conv2D
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Flatten
from keras.layers import Conv2D, MaxPooling2D
from keras.optimizers import adam
from keras import optimizers

from keras.models import Sequential

model = Sequential()

model.compile(loss='categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])

import tensorflow as tf
import subprocess
import webbrowser
import time

!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

model = Sequential()

# Premiere couche (couche d'entree)
model.add(Conv2D(filters=4, kernel_size=2, padding='same',
                 activation='relu', input_shape=(28, 28, 3)))
model.add(MaxPooling2D(pool_size=2))

# Deuxieme couche
model.add(Conv2D(filters=6, kernel_size=2, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(0.1))

#Troixieme couche
model.add(Conv2D(filters=8, kernel_size=2, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(0.2))


# Quatrieme couche
model.add(Conv2D(filters=12, kernel_size=2, padding='same', activation='relu'))
model.add(MaxPooling2D(pool_size=2))
model.add(Dropout(0.4))

# Cinquieme couche
model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dropout(128))

# Sixieme couche (couche de sortie/classification)
model.add(Dense(2, activation='softmax')) 


model.summary()

# Compilation du modele avant l'entrainement
#model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])

model.compile(optimizer=optimizers.adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

from keras.callbacks import ModelCheckpoint,EarlyStopping

# Sauvegarder le modèle avec les meilleurs poids
checkpointer = ModelCheckpoint('gdrive/My Drive/DEEP/model/model.h5', verbose=1,save_best_only=True)

# Arrêtez la formation si le modèle ne montre aucune amélioration 
stopper = EarlyStopping(monitor='val_loss',min_delta=0.1,patience=0,verbose=1,mode='auto')

history_model = model.fit_generator(train_generator, steps_per_epoch = 13,
                                    validation_data=valid_generator,
                                    validation_steps=3, 
                                    epochs=30,verbose=1,
                                    callbacks=[checkpointer])

!pip install statistics
import statistics

#model.evaluate_generator(generator=valid_generator, steps=3)
model.evaluate_generator(generator=valid_generator, steps=3)

model.save('gdrive/My Drive/DEEP/model/model.h5','w')

from keras.models import load_model
model = load_model('gdrive/My Drive/DEEP/model/model.h5') 
model.load_weights('gdrive/My Drive/DEEP/model/model.h5')

display(history_model.history)

import matplotlib.pyplot as plt
from matplotlib import*

import collections

def plot_training(history):
    acc = history_model.history['acc'] 
    val_acc = history.history['val_acc']
    loss = history_model.history['loss']
    val_loss = history.history['val_loss']
    epochs  = range(len(acc))
    
    
    plt.plot(epochs, acc, '-', color='orange', label='training acc')
    plt.plot(epochs, val_acc, '-', color='blue', label='validation acc')
    plt.title('Training and validation accuracy')
    plt.legend()
    plt.show()

    plt.plot(epochs, loss, '-', color='orange', label='training acc')
    plt.plot(epochs, val_loss,  '-', color='blue', label='validation acc')
    plt.title('Training and validation loss')
    plt.legend()
    plt.show()

import collections

plot_training(history_model)

from mlxtend.plotting import plot_confusion_matrix
import matplotlib.pyplot as plt
import numpy as np

binary = np.array([[2665, 708],
                   [721, 1765]])

fig, ax = plot_confusion_matrix(conf_mat=binary,
                                show_absolute=True,
                                show_normed=True,
                                colorbar=True)
plt.show()

def  precision_macro_average ( confusion_matrix ): 
    lignes ,  colonnes  =  confusion_matrix . forme 
    sum_of_precisions  =  0 
    pour l' étiquette  dans la  plage ( lignes ):
        sum_of_precisions  + =  precision ( label ,  confusion_matrix ) 
    renvoie  sum_of_precisions  /  rows 
def  rappel_macro_average ( confusion_matrix ): 
    lignes ,  colonnes  =  confusion_matrix . forme 
    sum_of_recalls  =  0 
    pour l'  étiquette  dans la  plage ( colonnes ): 
        sum_of_recalls  + =  rappel ( étiquette ,  confusion_matrix ) 
    return  sum_of_recalls  / des colonnes

# Chargez les poids à partir du modèle affiné
model.load_weights('gdrive/My Drive/DEEP/model/model.h5',)

# Commented out IPython magic to ensure Python compatibility.
from keras.preprocessing.image import img_to_array,load_img
import matplotlib.pyplot as plt
import cv2
# %matplotlib inline
def pred(img_path):    
    img = load_img(img_path,target_size = (299,299)) #Chargez l'image et réglez la taille cible sur la taille d'entrée de notre modèle
    x = img_to_array(img) # Convertir l'image en tableau
    x = np.expand_dims(x,axis=0) # Convertir le tableau à la forme (1, x, y, z)
    x = preprocess_input(x) # Utiliser la fonction d’entrée de prétraitement o soustraire la moyenne de toutes les images
    p = np.argmax(model.predict(x)) # Stocke l'argmax des prédictions
    if p==0:     # Si P=0 (NORMAL) , P=1 (BACTERIA),
        print("NORMAL")
    elif p==1:
        print("PNEUMONIA")

pred("gdrive/My Drive/DEEP/validation/Normal/N2.jpeg")
z = plt.imread('gdrive/My Drive/DEEP/validation/Pneumonia/B2.jpeg') 
plt.imshow(z);

pred("gdrive/My Drive/DEEP/validation/Pneumonia/V2.jpeg")
z = plt.imread('gdrive/My Drive/DEEP/validation/Pneumonia/V2.jpeg') 
plt.imshow(z);

pred("gdrive/My Drive/DEEP/validation/Pneumonia/V1.jpeg")
z = plt.imread('gdrive/My Drive/DEEP/validation/Pneumonia/V1.jpeg') 
plt.imshow(z);

pred("gdrive/My Drive/DEEP/validation/Pneumonia/B8.jpeg")
z = plt.imread('gdrive/My Drive/DEEP/validation/Pneumonia/B8.jpeg') 
plt.imshow(z);